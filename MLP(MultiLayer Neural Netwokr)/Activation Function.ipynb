{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60181f25-ec79-481b-b68d-3c0dabcc51a8",
   "metadata": {},
   "source": [
    "# üîã 1. Activation Functions\n",
    "They decide whether a neuron should be \"activated\" (fired) or not. It adds non-linearity to the model.\n",
    "\n",
    "## üî∏ Sigmoid:\n",
    "1. Formula: 1 / (1 + e^(-x))\n",
    "\n",
    "2. Output: between 0 and 1\n",
    "\n",
    "3. üîπ Use: Binary classification\n",
    "\n",
    "4. ‚ö†Ô∏è Problem: Causes vanishing gradient.\n",
    "\n",
    "## üî∏ ReLU (Rectified Linear Unit):\n",
    "1. Formula: f(x) = max(0, x)\n",
    "\n",
    "2. Output: 0 or x\n",
    "\n",
    "## üîπ Use: Default for hidden layers\n",
    "\n",
    "1. ‚úÖ Fast, avoids vanishing gradient\n",
    "\n",
    "2. ‚ö†Ô∏è Sometimes neurons die (output always 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce114db-58c7-48f5-a633-b5d68b0fba87",
   "metadata": {},
   "source": [
    "# üîÑ 2. Forward Propagation\n",
    "1. Data flows from input ‚Üí hidden layers ‚Üí output.\n",
    "\n",
    "2. At each neuron, apply:\n",
    "\n",
    "   1.  z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b\n",
    "   2. a = activation(z)\n",
    "3. Final output is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f465e-cea2-4c86-87ad-a73e2db0d279",
   "metadata": {},
   "source": [
    "# üîÅ 3. Backward Propagation (Backprop)\n",
    "1. After prediction, we calculate the error/loss.\n",
    "\n",
    "2. Backpropagation helps adjust weights to reduce error:\n",
    "\n",
    "    1. Compute gradients using chain rule of derivatives\n",
    "\n",
    "    2. Update weights from output layer back to input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c611f-1dbb-48c6-9934-e4ff9e6f43d2",
   "metadata": {},
   "source": [
    "# üìâ 4. Gradient Descent\n",
    "It‚Äôs an algorithm used to minimize the loss by updating weights in the opposite direction of the gradient.\n",
    "\n",
    "## üîπ Formula:\n",
    "new_weight = old_weight - learning_rate √ó gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f9d54-840c-4803-9b78-f6d2d129b9c6",
   "metadata": {},
   "source": [
    "# üí• 5. Loss Function\n",
    "It measures how wrong the model is.\n",
    "\n",
    "## Common Types:\n",
    "1. Mean Squared Error (MSE)(Regression tasks)\n",
    "2. Binary Cross Entropy(Binary classification)\n",
    "3. Categorical Cross Entropy(Multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ff32a-5911-4761-a196-2259cc118f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
