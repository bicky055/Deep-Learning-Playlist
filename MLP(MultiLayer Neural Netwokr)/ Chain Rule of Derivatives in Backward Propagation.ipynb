{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc48b5b5-5a63-4ad5-b22a-7114d08002ba",
   "metadata": {},
   "source": [
    " # Chain Rule of Derivatives in Backward Propagation\n",
    " The chain rule is the core math behind backward propagation in neural networks. It allows us to compute how the loss function changes with respect to each weight by breaking down the complex derivatives into smaller parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69fd17-cb17-4b30-aae6-21d88849cbe7",
   "metadata": {},
   "source": [
    "## 🔍 Why We Need Chain Rule in Backprop:\n",
    "In a multilayer network, output depends on the composition of multiple functions:\n",
    "\n",
    "Loss → Output → Activation → Weighted sum → Inputs\n",
    "\n",
    "To find how loss changes with respect to each weight (∂L/∂w), we use the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cffbb-7409-4ac0-ae7f-a3f8ef3ab682",
   "metadata": {},
   "source": [
    "## 🔣 Chain Rule (Basic Form):\n",
    "If\n",
    "\n",
    "y = f(g(x)),  \n",
    "then dy/dx = f'(g(x)) × g'(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef833d-31be-4d87-86e7-abbadd45c69f",
   "metadata": {},
   "source": [
    "## 📘 In Backpropagation (Simple Example):\n",
    "Let’s say:\n",
    "\n",
    "z = w·x + b (linear transformation)\n",
    "\n",
    "a = activation(z) (like sigmoid or ReLU)\n",
    "\n",
    "L = loss(a, y) (loss function)\n",
    "\n",
    "To update w, we need:\n",
    "\n",
    "∂L/∂w = ∂L/∂a × ∂a/∂z × ∂z/∂w\n",
    "\n",
    "Each term is a small derivative:\n",
    "\n",
    "   1. ∂L/∂a: How loss changes with output\n",
    "\n",
    "   2. ∂a/∂z: How output changes with activation\n",
    "\n",
    "   3. ∂z/∂w: How z changes with weight (this is x)\n",
    "\n",
    "This is exactly the chain rule in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a00b91-873d-41a3-99ec-0855ba808dac",
   "metadata": {},
   "source": [
    "## 🔗 Backpropagation Uses Chain Rule Layer-by-Layer:\n",
    "In deeper networks:\n",
    "\n",
    "∂L/∂w₁ = ∂L/∂a₃ × ∂a₃/∂z₃ × ∂z₃/∂a₂ × ∂a₂/∂z₂ × ∂z₂/∂a₁ × ∂a₁/∂z₁ × ∂z₁/∂w₁\n",
    "\n",
    "It continues backwards, applying the chain rule from output to input — hence the name backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b4150-2d62-43d7-857c-9feea882f6c9",
   "metadata": {},
   "source": [
    "## ✅ Summary:\n",
    "The chain rule helps break down a complicated derivative into small, manageable pieces.\n",
    "\n",
    "It’s essential in updating weights during training.\n",
    "\n",
    "Without it, deep learning wouldn't work!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f99eaf-0f3f-46ca-b0f0-e619f7dd4185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
