{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4e7f38-e7f6-483a-93c3-9cdd1fb4e924",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Problem in Multilayered Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eda2f6-2e94-4f7a-aa9a-c1d33030b481",
   "metadata": {},
   "source": [
    "## üß† What is it?\n",
    "The vanishing gradient problem occurs during backpropagation in deep neural networks when the gradients (errors) become very small as they are passed backward through layers.\n",
    "As a result:\n",
    "\n",
    "   1. Earlier layers learn very slowly or not at all.\n",
    "\n",
    "   2. The network fails to converge or becomes very hard to train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2bf9a-75fe-410d-a107-7391d7ef59d6",
   "metadata": {},
   "source": [
    "## üîÅ Why Does It Happen?\n",
    "It‚Äôs due to repeated multiplication of small derivatives (< 1) using the chain rule in backpropagation.\n",
    "\n",
    "Example:\n",
    "\n",
    "If in each layer the gradient is 0.5, and we have 10 layers:\n",
    "\n",
    "Total gradient = 0.5^10 ‚âà 0.00098 ‚Üí Almost zero\n",
    "\n",
    "This leads to almost no weight update in early layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07314af0-d255-4bbb-b6bd-2b407fe686e4",
   "metadata": {},
   "source": [
    "## üîç Mostly Happens With:\n",
    "Deep networks (many hidden layers)\n",
    "\n",
    "Sigmoid or tanh activation functions\n",
    "\n",
    "    1. Their gradients are < 1 and can shrink fast.\n",
    "\n",
    "    2. Near 0 or 1, derivatives become close to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3aa54-e86f-42a4-b348-a188f423a311",
   "metadata": {},
   "source": [
    "## ‚ùå Effect:\n",
    "Weights in early layers stop learning.\n",
    "\n",
    "Training becomes slow or stuck.\n",
    "\n",
    "Final model performs poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a328d-f54f-4635-97b2-10fb8923712a",
   "metadata": {},
   "source": [
    "## ‚úÖ Solutions to Vanishing Gradient:\n",
    "\n",
    "We used other Activation Function like\n",
    "\n",
    "1. ReLU Activation (ReLU does not squash gradients like sigmoid/tanh.)\n",
    "\n",
    "2. Batch Normalization (Keeps activations in a stable range.)\n",
    "\n",
    "3. He or Xavier Initialization(Smart ways to initialize weights to prevent gradient shrink.)\n",
    "\n",
    "4. Skip Connections (ResNet)\t(Allows gradient to flow directly to earlier layers.)\n",
    "\n",
    "5. LSTM/GRU (in RNNs)\t(Designed to retain gradients better than vanilla RNNs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91423e-ab49-488b-82f1-43364a475af7",
   "metadata": {},
   "source": [
    "## üìå Summary:\n",
    "Vanishing gradient = gradient becomes too small ‚Üí early layers stop learning ‚Üí deep networks fail to train properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c72326-569f-43b1-9cbd-7d6fdb77baf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
